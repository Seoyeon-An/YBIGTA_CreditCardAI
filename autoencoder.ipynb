{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.optim import Optimizer, AdamW\n",
    "from torch.optim.lr_scheduler import LambdaLR, CyclicLR, OneCycleLR\n",
    "\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.device'>\n"
     ]
    }
   ],
   "source": [
    "print(type(device))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EPOCHS: 전체 train dataset에 대하여 얼마나 training 할 것인가 <br>\n",
    "LR(learning rate): step size라고도 하며 gradient descent 할 때 얼마나 변경할 것인가 <br>\n",
    "BS(batch size): batch 1개에 dataset의 개수<br>\n",
    "SEED: random 값을 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "LR = 1e-2\n",
    "BS = 16384 # 2의 제곱승꼴\n",
    "SEED = 41"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(SEED) # Seed 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./data/train.csv')\n",
    "train_df = train_df.drop(columns=['ID'])\n",
    "# train_df = train_df[['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V9', 'V10', 'V11', 'V12', 'V14', 'V16', 'V17', 'V18', 'V30']]\n",
    "val_df = pd.read_csv('./data/val.csv')\n",
    "val_df = val_df.drop(columns=['ID'])\n",
    "# val_df = val_df[['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V9', 'V10', 'V11', 'V12', 'V14', 'V16', 'V17', 'V18', 'V30', 'Class']]\n",
    "test_df = pd.read_csv('./data/test.csv')\n",
    "test_df = test_df.drop(columns=['ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, df, eval_mode):\n",
    "        self.df = df\n",
    "        self.eval_mode = eval_mode\n",
    "        if self.eval_mode:\n",
    "            self.labels = self.df['Class'].values\n",
    "            self.df = self.df.drop(columns=['Class']).values\n",
    "        else:\n",
    "            self.df = self.df.values\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        if self.eval_mode:\n",
    "            x = torch.from_numpy(self.df[index]).type(torch.FloatTensor)\n",
    "            y = torch.FloatTensor([self.labels[index]])\n",
    "            return x, y\n",
    "            #self.x = self.df[index]\n",
    "            #self.y = self.labels[index]\n",
    "            #return torch.Tensor(self.x), self.y\n",
    "        else:\n",
    "            self.x = self.df[index]\n",
    "            return torch.Tensor(self.x)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MyDataset(df=train_df, eval_mode=False)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BS, shuffle=True, num_workers=6)\n",
    "\n",
    "val_dataset = MyDataset(df=val_df, eval_mode=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BS, shuffle=False, num_workers=6)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model architecture\n",
    "원래 Autoencoder는 차원 축소 후 복원이지만 이번 task는 input dimension이 30 밖에 안되기 때문에 encoder로 차원을 확대한 후 decoder로 차원을 줄여 복원하는 방식으로 구현"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chage hyperparameters in Neural Network\n",
    "1. LeakyReLU -> GELU </br>\n",
    "    최고 macro f1 score까지 더 빨리 왔지만 더이상 올라가지 않음 <br>\n",
    "    suppose: 아마 오답에 매우 가까운 정답이 있거나 정답에 매우 가까운 오답이 있음, 전자일 확률이 높음<br>\n",
    "    A: 정답에 매우 가까운 오답들이 너무 많음....(feat. EDA) \n",
    "    \n",
    "2. Linear layer를 더 늘려보자 =>  macro f1-score가 올라가다가 다시 내려감 결국 50으로...\n",
    "3. threshold를 더 늘려보자. 95% -> 98% => 85% 정도에서 고정..\n",
    "4. 원래 방식대로 차원을 줄이고 나서 늘려볼까? => 성능 더 안 좋음..\n",
    "5. diff를 cosinesimilarity -> ts-ss => 더 안 좋음.. <br>\n",
    "    Linear layer를 더 늘려보니 더 안 좋음..<br>\n",
    "    ts-ss가 이번 task에 잘 맞지 않는 이유 추정: cosine similarity 보다 두 벡터 사이의 차이를 더 정밀하게 나타내 주지만 너무 정밀하게 나타내어 정답의 threshold를 정하기 힘듦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 계층 정규화\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, hidden_size, eps=1e-5):\n",
    "        \"\"\"Construct a layernorm module in the TF style (epsilon inside the square root).\n",
    "        \"\"\"\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
    "        self.bias = nn.Parameter(torch.zeros(hidden_size))\n",
    "        self.variance_epsilon = eps\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.weight.data.fill_(1.0)\n",
    "        self.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        u = x.mean(-1, keepdim=True)\n",
    "        s = (x - u).pow(2).mean(-1, keepdim=True)\n",
    "        x = (x - u) / torch.sqrt(s + self.variance_epsilon) # 계층정규화 완료\n",
    "        return self.weight * x + self.bias # wx+b\n",
    "        \n",
    "# 활성화 함수\n",
    "class GELU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "        \n",
    "class AutoEncoder1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AutoEncoder1, self).__init__()\n",
    "        \n",
    "        \n",
    "        self.ln = LayerNorm(5000)\n",
    "        self.ln1 = LayerNorm(3500)\n",
    "        self.ln2 = LayerNorm(2000)\n",
    "        self.ln3 = LayerNorm(1000)\n",
    "        \n",
    "        self.upblock1 = nn.Sequential(nn.Linear(30, 1000), nn.BatchNorm1d(1000),GELU())\n",
    "        self.upblock2 = nn.Sequential(nn.Linear(1000,2000), nn.BatchNorm1d(2000),GELU())\n",
    "        self.upblock3 = nn.Sequential(nn.Linear(2000,3500), nn.BatchNorm1d(3500),GELU())\n",
    "        self.upblock4 = nn.Sequential(nn.Linear(3500,5000), nn.BatchNorm1d(5000),GELU())\n",
    "\n",
    "        self.downblock1 = nn.Sequential(nn.Linear(5000, 3500),nn.BatchNorm1d(3500),GELU())\n",
    "        self.downblock2 = nn.Sequential(nn.Linear(3500, 2000),nn.BatchNorm1d(2000),GELU())\n",
    "        self.downblock3 = nn.Sequential(nn.Linear(2000, 1000),nn.BatchNorm1d(1000),GELU())\n",
    "        self.downblock4 = nn.Sequential(nn.Linear(1000, 300),nn.BatchNorm1d(300),GELU())\n",
    "        \n",
    "        self.fclayer = nn.Sequential(nn.Linear(300,30))\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        upblock1_out = self.upblock1(x) \n",
    "        upblock2_out = self.upblock2(upblock1_out)\n",
    "        upblock3_out = self.upblock3(upblock2_out)\n",
    "        upblock4_out = self.upblock4(upblock3_out)\n",
    "        \n",
    "        downblock1_out = self.downblock1(self.ln(upblock4_out)) \n",
    "        skipblock1 = downblock1_out + upblock3_out\n",
    "        downblock2_out = self.downblock2(self.ln1(skipblock1))\n",
    "        skipblock2 = downblock2_out + upblock2_out\n",
    "        downblock3_out = self.downblock3(self.ln2(skipblock2))\n",
    "        skipblock3 = downblock3_out + upblock1_out \n",
    "        downblock4_out = self.downblock4(self.ln3(skipblock3))\n",
    "        \n",
    "        x = self.fclayer(downblock4_out)\n",
    "         \n",
    "        return x # 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.vector_sim import TS_SS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self, model, optimizer, train_loader, val_loader, scheduler, device):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.scheduler = scheduler\n",
    "        self.device = device\n",
    "        # Loss Function\n",
    "        self.criterion = nn.L1Loss().to(self.device)\n",
    "        \n",
    "    def fit(self):\n",
    "        self.model.to(self.device)\n",
    "        best_score = 0\n",
    "        avg = 1\n",
    "        for epoch in range(EPOCHS):\n",
    "            self.model.train()\n",
    "            train_loss = []\n",
    "            for x in iter(self.train_loader):\n",
    "                x = x.float().to(self.device)\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                _x = self.model(x)\n",
    "                loss = self.criterion(x, _x)\n",
    "\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                train_loss.append(loss.item())\n",
    "\n",
    "            score = self.validation(self.model, 0.95)\n",
    "            print(f'Epoch : [{epoch}] Train loss : [{np.mean(train_loss)}] Val Score : [{score}])')\n",
    "\n",
    "            if self.scheduler is not None:\n",
    "                self.scheduler.step(score)\n",
    "\n",
    "            if best_score <= score and avg > np.mean(train_loss):\n",
    "                best_score = score\n",
    "                avg = np.mean(train_loss)\n",
    "                torch.save(model.module.state_dict(), './best_model.pth', _use_new_zipfile_serialization=False)\n",
    "                print('---------------------------')\n",
    "                print(f'Train loss : [{np.mean(train_loss)}] Val Score : [{score}])')\n",
    "    \n",
    "    def validation(self, eval_model, thr):\n",
    "        cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "        eval_model.eval()\n",
    "        pred = []\n",
    "        true = []\n",
    "        with torch.no_grad():\n",
    "            for x, y in iter(self.val_loader):\n",
    "                x = x.float().to(self.device)\n",
    "\n",
    "                _x = self.model(x)\n",
    "                diff = cos(x, _x).cpu().tolist()\n",
    "                batch_pred = np.where(np.array(diff)<thr, 1, 0).tolist()\n",
    "                pred += batch_pred\n",
    "                true += y.tolist()\n",
    "\n",
    "        return f1_score(true, pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [0] Train loss : [0.5132853133337838] Val Score : [0.02092859841809397])\n",
      "---------------------------\n",
      "Train loss : [0.5132853133337838] Val Score : [0.02092859841809397])\n",
      "Epoch : [1] Train loss : [0.2488231701510293] Val Score : [0.2906414832196451])\n",
      "---------------------------\n",
      "Train loss : [0.2488231701510293] Val Score : [0.2906414832196451])\n",
      "Epoch : [2] Train loss : [0.1722170923437391] Val Score : [0.48495470474700497])\n",
      "---------------------------\n",
      "Train loss : [0.1722170923437391] Val Score : [0.48495470474700497])\n",
      "Epoch : [3] Train loss : [0.13155716338327952] Val Score : [0.506937276930102])\n",
      "---------------------------\n",
      "Train loss : [0.13155716338327952] Val Score : [0.506937276930102])\n",
      "Epoch : [4] Train loss : [0.10789042604821068] Val Score : [0.5114884236319622])\n",
      "---------------------------\n",
      "Train loss : [0.10789042604821068] Val Score : [0.5114884236319622])\n",
      "Epoch : [5] Train loss : [0.09442150805677686] Val Score : [0.5191761322546462])\n",
      "---------------------------\n",
      "Train loss : [0.09442150805677686] Val Score : [0.5191761322546462])\n",
      "Epoch : [6] Train loss : [0.08551350235939026] Val Score : [0.5419510919830159])\n",
      "---------------------------\n",
      "Train loss : [0.08551350235939026] Val Score : [0.5419510919830159])\n",
      "Epoch : [7] Train loss : [0.07975502950804574] Val Score : [0.5556525296376691])\n",
      "---------------------------\n",
      "Train loss : [0.07975502950804574] Val Score : [0.5556525296376691])\n",
      "Epoch : [8] Train loss : [0.07796390673943929] Val Score : [0.5723410221437316])\n",
      "---------------------------\n",
      "Train loss : [0.07796390673943929] Val Score : [0.5723410221437316])\n",
      "Epoch : [9] Train loss : [0.07534083191837583] Val Score : [0.5862824086461611])\n",
      "---------------------------\n",
      "Train loss : [0.07534083191837583] Val Score : [0.5862824086461611])\n",
      "Epoch : [10] Train loss : [0.07359852854694639] Val Score : [0.6243147868031209])\n",
      "---------------------------\n",
      "Train loss : [0.07359852854694639] Val Score : [0.6243147868031209])\n",
      "Epoch : [11] Train loss : [0.07349861626114164] Val Score : [0.7059866032567539])\n",
      "---------------------------\n",
      "Train loss : [0.07349861626114164] Val Score : [0.7059866032567539])\n",
      "Epoch : [12] Train loss : [0.07186735634292875] Val Score : [0.8045965667777433])\n",
      "---------------------------\n",
      "Train loss : [0.07186735634292875] Val Score : [0.8045965667777433])\n",
      "Epoch : [13] Train loss : [0.06854273911033358] Val Score : [0.8202665410912253])\n",
      "---------------------------\n",
      "Train loss : [0.06854273911033358] Val Score : [0.8202665410912253])\n",
      "Epoch : [14] Train loss : [0.06422143961702075] Val Score : [0.8519279892324237])\n",
      "---------------------------\n",
      "Train loss : [0.06422143961702075] Val Score : [0.8519279892324237])\n",
      "Epoch : [15] Train loss : [0.06212137958833149] Val Score : [0.8621517488551477])\n",
      "---------------------------\n",
      "Train loss : [0.06212137958833149] Val Score : [0.8621517488551477])\n",
      "Epoch : [16] Train loss : [0.05950168892741203] Val Score : [0.8967110829723166])\n",
      "---------------------------\n",
      "Train loss : [0.05950168892741203] Val Score : [0.8967110829723166])\n",
      "Epoch : [17] Train loss : [0.05715276354125568] Val Score : [0.9031202878275757])\n",
      "---------------------------\n",
      "Train loss : [0.05715276354125568] Val Score : [0.9031202878275757])\n",
      "Epoch : [18] Train loss : [0.054662130773067474] Val Score : [0.9031202878275757])\n",
      "---------------------------\n",
      "Train loss : [0.054662130773067474] Val Score : [0.9031202878275757])\n",
      "Epoch : [19] Train loss : [0.053609508488859446] Val Score : [0.9031202878275757])\n",
      "---------------------------\n",
      "Train loss : [0.053609508488859446] Val Score : [0.9031202878275757])\n",
      "Epoch : [20] Train loss : [0.054117935576609204] Val Score : [0.9031202878275757])\n",
      "Epoch : [21] Train loss : [0.052513149699994495] Val Score : [0.9097393418694286])\n",
      "---------------------------\n",
      "Train loss : [0.052513149699994495] Val Score : [0.9097393418694286])\n",
      "Epoch : [22] Train loss : [0.05222963701401438] Val Score : [0.9097393418694286])\n",
      "---------------------------\n",
      "Train loss : [0.05222963701401438] Val Score : [0.9097393418694286])\n",
      "Epoch : [23] Train loss : [0.052602804132870266] Val Score : [0.9097393418694286])\n",
      "Epoch : [24] Train loss : [0.05074095885668482] Val Score : [0.9097393418694286])\n",
      "---------------------------\n",
      "Train loss : [0.05074095885668482] Val Score : [0.9097393418694286])\n",
      "Epoch : [25] Train loss : [0.048588638859135766] Val Score : [0.9165787375726882])\n",
      "---------------------------\n",
      "Train loss : [0.048588638859135766] Val Score : [0.9165787375726882])\n",
      "Epoch : [26] Train loss : [0.0464845709502697] Val Score : [0.9165787375726882])\n",
      "---------------------------\n",
      "Train loss : [0.0464845709502697] Val Score : [0.9165787375726882])\n",
      "Epoch : [27] Train loss : [0.04562926931040628] Val Score : [0.9165787375726882])\n",
      "---------------------------\n",
      "Train loss : [0.04562926931040628] Val Score : [0.9165787375726882])\n",
      "Epoch : [28] Train loss : [0.04636801353522709] Val Score : [0.9165787375726882])\n",
      "Epoch : [29] Train loss : [0.04546053547944341] Val Score : [0.9165787375726882])\n",
      "---------------------------\n",
      "Train loss : [0.04546053547944341] Val Score : [0.9165787375726882])\n",
      "Epoch : [30] Train loss : [0.04494435180510793] Val Score : [0.9165787375726882])\n",
      "---------------------------\n",
      "Train loss : [0.04494435180510793] Val Score : [0.9165787375726882])\n",
      "Epoch : [31] Train loss : [0.044525442378861566] Val Score : [0.9165787375726882])\n",
      "---------------------------\n",
      "Train loss : [0.044525442378861566] Val Score : [0.9165787375726882])\n",
      "Epoch : [32] Train loss : [0.04399423833404269] Val Score : [0.9165787375726882])\n",
      "---------------------------\n",
      "Train loss : [0.04399423833404269] Val Score : [0.9165787375726882])\n",
      "Epoch : [33] Train loss : [0.043977675693375726] Val Score : [0.9165787375726882])\n",
      "---------------------------\n",
      "Train loss : [0.043977675693375726] Val Score : [0.9165787375726882])\n",
      "Epoch : [34] Train loss : [0.042053718119859695] Val Score : [0.9165787375726882])\n",
      "---------------------------\n",
      "Train loss : [0.042053718119859695] Val Score : [0.9165787375726882])\n",
      "Epoch : [35] Train loss : [0.042536549270153046] Val Score : [0.9165787375726882])\n",
      "Epoch : [36] Train loss : [0.042617046407290866] Val Score : [0.9165787375726882])\n",
      "Epoch 00037: reducing learning rate of group 0 to 5.0000e-03.\n",
      "Epoch : [37] Train loss : [0.03658338103975568] Val Score : [0.9165787375726882])\n",
      "---------------------------\n",
      "Train loss : [0.03658338103975568] Val Score : [0.9165787375726882])\n",
      "Epoch : [38] Train loss : [0.03428610733577183] Val Score : [0.9165787375726882])\n",
      "---------------------------\n",
      "Train loss : [0.03428610733577183] Val Score : [0.9165787375726882])\n",
      "Epoch : [39] Train loss : [0.03433103061148098] Val Score : [0.9165787375726882])\n",
      "Epoch : [40] Train loss : [0.03264120753322329] Val Score : [0.9165787375726882])\n",
      "---------------------------\n",
      "Train loss : [0.03264120753322329] Val Score : [0.9165787375726882])\n",
      "Epoch : [41] Train loss : [0.03266616538167] Val Score : [0.9165787375726882])\n",
      "Epoch : [42] Train loss : [0.032676676022154946] Val Score : [0.9165787375726882])\n",
      "Epoch : [43] Train loss : [0.03248077684215137] Val Score : [0.9165787375726882])\n",
      "---------------------------\n",
      "Train loss : [0.03248077684215137] Val Score : [0.9165787375726882])\n",
      "Epoch : [44] Train loss : [0.03277915237205369] Val Score : [0.9165787375726882])\n",
      "Epoch : [45] Train loss : [0.03352832554706505] Val Score : [0.9165787375726882])\n",
      "Epoch : [46] Train loss : [0.033036816865205765] Val Score : [0.9165787375726882])\n",
      "Epoch : [47] Train loss : [0.03242506432746138] Val Score : [0.9165787375726882])\n",
      "Epoch 00048: reducing learning rate of group 0 to 2.5000e-03.\n",
      "---------------------------\n",
      "Train loss : [0.03242506432746138] Val Score : [0.9165787375726882])\n",
      "Epoch : [48] Train loss : [0.030219643243721554] Val Score : [0.9165787375726882])\n",
      "---------------------------\n",
      "Train loss : [0.030219643243721554] Val Score : [0.9165787375726882])\n",
      "Epoch : [49] Train loss : [0.028185551187821796] Val Score : [0.9165787375726882])\n",
      "---------------------------\n",
      "Train loss : [0.028185551187821796] Val Score : [0.9165787375726882])\n",
      "Epoch : [50] Train loss : [0.029355183243751526] Val Score : [0.9165787375726882])\n",
      "Epoch : [51] Train loss : [0.028682525668825423] Val Score : [0.9165787375726882])\n",
      "Epoch : [52] Train loss : [0.028851859537618502] Val Score : [0.9165787375726882])\n",
      "Epoch : [53] Train loss : [0.027889846957155635] Val Score : [0.9165787375726882])\n",
      "---------------------------\n",
      "Train loss : [0.027889846957155635] Val Score : [0.9165787375726882])\n",
      "Epoch : [54] Train loss : [0.027380540700895444] Val Score : [0.9165787375726882])\n",
      "---------------------------\n",
      "Train loss : [0.027380540700895444] Val Score : [0.9165787375726882])\n",
      "Epoch : [55] Train loss : [0.027512636301772937] Val Score : [0.9165787375726882])\n",
      "Epoch : [56] Train loss : [0.027872327182974135] Val Score : [0.9165787375726882])\n",
      "Epoch : [57] Train loss : [0.027687275782227516] Val Score : [0.9165787375726882])\n",
      "Epoch : [58] Train loss : [0.028265260958245823] Val Score : [0.9165787375726882])\n",
      "Epoch 00059: reducing learning rate of group 0 to 1.2500e-03.\n",
      "Epoch : [59] Train loss : [0.02629242225417069] Val Score : [0.9165787375726882])\n",
      "---------------------------\n",
      "Train loss : [0.02629242225417069] Val Score : [0.9165787375726882])\n",
      "Epoch : [60] Train loss : [0.02499014777796609] Val Score : [0.9165787375726882])\n",
      "---------------------------\n",
      "Train loss : [0.02499014777796609] Val Score : [0.9165787375726882])\n",
      "Epoch : [61] Train loss : [0.02533117202775819] Val Score : [0.9165787375726882])\n",
      "Epoch : [62] Train loss : [0.024954737297126224] Val Score : [0.9165787375726882])\n",
      "---------------------------\n",
      "Train loss : [0.024954737297126224] Val Score : [0.9165787375726882])\n",
      "Epoch : [63] Train loss : [0.025438199351940836] Val Score : [0.9165787375726882])\n",
      "Epoch : [64] Train loss : [0.025074176490306854] Val Score : [0.9165787375726882])\n",
      "Epoch : [65] Train loss : [0.025571051452841078] Val Score : [0.9165787375726882])\n",
      "Epoch : [66] Train loss : [0.02517812299941267] Val Score : [0.9165787375726882])\n",
      "Epoch : [67] Train loss : [0.024470527257238115] Val Score : [0.9165787375726882])\n",
      "---------------------------\n",
      "Train loss : [0.024470527257238115] Val Score : [0.9165787375726882])\n",
      "Epoch : [68] Train loss : [0.024932878091931343] Val Score : [0.9165787375726882])\n",
      "Epoch : [69] Train loss : [0.02560196311346122] Val Score : [0.9165787375726882])\n",
      "Epoch 00070: reducing learning rate of group 0 to 6.2500e-04.\n",
      "Epoch : [70] Train loss : [0.02454291337302753] Val Score : [0.9165787375726882])\n",
      "Epoch : [71] Train loss : [0.02401830761560372] Val Score : [0.9165787375726882])\n",
      "---------------------------\n",
      "Train loss : [0.02401830761560372] Val Score : [0.9165787375726882])\n",
      "Epoch : [72] Train loss : [0.023738409525581768] Val Score : [0.9165787375726882])\n",
      "---------------------------\n",
      "Train loss : [0.023738409525581768] Val Score : [0.9165787375726882])\n",
      "Epoch : [73] Train loss : [0.02363461601947035] Val Score : [0.9165787375726882])\n",
      "---------------------------\n",
      "Train loss : [0.02363461601947035] Val Score : [0.9165787375726882])\n",
      "Epoch : [74] Train loss : [0.02378029323049954] Val Score : [0.9165787375726882])\n",
      "Epoch : [75] Train loss : [0.023812218702265193] Val Score : [0.9165787375726882])\n",
      "Epoch : [76] Train loss : [0.024399434881550924] Val Score : [0.9165787375726882])\n",
      "Epoch : [77] Train loss : [0.024159438642007963] Val Score : [0.9165787375726882])\n",
      "Epoch : [78] Train loss : [0.023750692073787962] Val Score : [0.9165787375726882])\n",
      "Epoch : [79] Train loss : [0.024394907589469637] Val Score : [0.9165787375726882])\n",
      "Epoch : [80] Train loss : [0.02379702909716538] Val Score : [0.9165787375726882])\n",
      "Epoch 00081: reducing learning rate of group 0 to 3.1250e-04.\n",
      "Epoch : [81] Train loss : [0.023876270279288292] Val Score : [0.9165787375726882])\n",
      "Epoch : [82] Train loss : [0.023135204666427205] Val Score : [0.9165787375726882])\n",
      "---------------------------\n",
      "Train loss : [0.023135204666427205] Val Score : [0.9165787375726882])\n",
      "Epoch : [83] Train loss : [0.02333498825984342] Val Score : [0.9165787375726882])\n",
      "Epoch : [84] Train loss : [0.023162512640867914] Val Score : [0.9165787375726882])\n",
      "Epoch : [85] Train loss : [0.02361035213938781] Val Score : [0.9165787375726882])\n",
      "Epoch : [86] Train loss : [0.02312311450285571] Val Score : [0.9165787375726882])\n",
      "---------------------------\n",
      "Train loss : [0.02312311450285571] Val Score : [0.9165787375726882])\n",
      "Epoch : [87] Train loss : [0.022894942068627903] Val Score : [0.9165787375726882])\n",
      "---------------------------\n",
      "Train loss : [0.022894942068627903] Val Score : [0.9165787375726882])\n",
      "Epoch : [88] Train loss : [0.023239624287400926] Val Score : [0.9165787375726882])\n",
      "Epoch : [89] Train loss : [0.023345670263682092] Val Score : [0.9165787375726882])\n",
      "Epoch : [90] Train loss : [0.02371194532939366] Val Score : [0.9165787375726882])\n",
      "Epoch : [91] Train loss : [0.02355042871619974] Val Score : [0.9165787375726882])\n",
      "Epoch 00092: reducing learning rate of group 0 to 1.5625e-04.\n",
      "Epoch : [92] Train loss : [0.02262553120298045] Val Score : [0.9165787375726882])\n",
      "---------------------------\n",
      "Train loss : [0.02262553120298045] Val Score : [0.9165787375726882])\n",
      "Epoch : [93] Train loss : [0.022388756009084836] Val Score : [0.9165787375726882])\n",
      "---------------------------\n",
      "Train loss : [0.022388756009084836] Val Score : [0.9165787375726882])\n",
      "Epoch : [94] Train loss : [0.02254615431385381] Val Score : [0.9165787375726882])\n",
      "Epoch : [95] Train loss : [0.022894499823451042] Val Score : [0.9165787375726882])\n",
      "Epoch : [96] Train loss : [0.02237537396805627] Val Score : [0.9165787375726882])\n",
      "---------------------------\n",
      "Train loss : [0.02237537396805627] Val Score : [0.9165787375726882])\n",
      "Epoch : [97] Train loss : [0.0227147535021816] Val Score : [0.9165787375726882])\n",
      "Epoch : [98] Train loss : [0.022784590987222537] Val Score : [0.9165787375726882])\n",
      "Epoch : [99] Train loss : [0.023087225322212492] Val Score : [0.9165787375726882])\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "model = nn.DataParallel(AutoEncoder1())\n",
    "model.eval()\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr = LR)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=10, threshold_mode='abs', min_lr=1e-8, verbose=True)\n",
    "trainer = Trainer(model, optimizer, train_loader, val_loader, scheduler, device)\n",
    "trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'|===========================================================================|\\n|                  PyTorch CUDA memory summary, device ID 0                 |\\n|---------------------------------------------------------------------------|\\n|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\\n|===========================================================================|\\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\\n|---------------------------------------------------------------------------|\\n| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |\\n|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\\n|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\\n|---------------------------------------------------------------------------|\\n| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |\\n|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\\n|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\\n|---------------------------------------------------------------------------|\\n| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |\\n|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\\n|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\\n|---------------------------------------------------------------------------|\\n| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |\\n|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\\n|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\\n|---------------------------------------------------------------------------|\\n| Allocations           |       0    |       0    |       0    |       0    |\\n|       from large pool |       0    |       0    |       0    |       0    |\\n|       from small pool |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Active allocs         |       0    |       0    |       0    |       0    |\\n|       from large pool |       0    |       0    |       0    |       0    |\\n|       from small pool |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| GPU reserved segments |       0    |       0    |       0    |       0    |\\n|       from large pool |       0    |       0    |       0    |       0    |\\n|       from small pool |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Non-releasable allocs |       0    |       0    |       0    |       0    |\\n|       from large pool |       0    |       0    |       0    |       0    |\\n|       from small pool |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Oversize allocations  |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Oversize GPU segments |       0    |       0    |       0    |       0    |\\n|===========================================================================|\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_summary(device=None, abbreviated=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): AutoEncoder1(\n",
       "    (ln): LayerNorm()\n",
       "    (ln1): LayerNorm()\n",
       "    (ln2): LayerNorm()\n",
       "    (ln3): LayerNorm()\n",
       "    (upblock1): Sequential(\n",
       "      (0): Linear(in_features=30, out_features=1000, bias=True)\n",
       "      (1): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): GELU()\n",
       "    )\n",
       "    (upblock2): Sequential(\n",
       "      (0): Linear(in_features=1000, out_features=2000, bias=True)\n",
       "      (1): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): GELU()\n",
       "    )\n",
       "    (upblock3): Sequential(\n",
       "      (0): Linear(in_features=2000, out_features=3500, bias=True)\n",
       "      (1): BatchNorm1d(3500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): GELU()\n",
       "    )\n",
       "    (upblock4): Sequential(\n",
       "      (0): Linear(in_features=3500, out_features=5000, bias=True)\n",
       "      (1): BatchNorm1d(5000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): GELU()\n",
       "    )\n",
       "    (downblock1): Sequential(\n",
       "      (0): Linear(in_features=5000, out_features=3500, bias=True)\n",
       "      (1): BatchNorm1d(3500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): GELU()\n",
       "    )\n",
       "    (downblock2): Sequential(\n",
       "      (0): Linear(in_features=3500, out_features=2000, bias=True)\n",
       "      (1): BatchNorm1d(2000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): GELU()\n",
       "    )\n",
       "    (downblock3): Sequential(\n",
       "      (0): Linear(in_features=2000, out_features=1000, bias=True)\n",
       "      (1): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): GELU()\n",
       "    )\n",
       "    (downblock4): Sequential(\n",
       "      (0): Linear(in_features=1000, out_features=300, bias=True)\n",
       "      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): GELU()\n",
       "    )\n",
       "    (fclayer): Sequential(\n",
       "      (0): Linear(in_features=300, out_features=30, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoEncoder1()\n",
    "model.load_state_dict(torch.load('./checkpoint/best_model.pth'))\n",
    "model = nn.DataParallel(model)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test_dataset \u001b[39m=\u001b[39m MyDataset(test_df, \u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m      2\u001b[0m test_loader \u001b[39m=\u001b[39m DataLoader(test_dataset, batch_size\u001b[39m=\u001b[39mBS, shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, num_workers\u001b[39m=\u001b[39m\u001b[39m6\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_df' is not defined"
     ]
    }
   ],
   "source": [
    "test_dataset = MyDataset(test_df, False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BS, shuffle=False, num_workers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(model, thr, test_loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "    pred = []\n",
    "    with torch.no_grad():\n",
    "        for x in iter(test_loader):\n",
    "            x = x.float().to(device)\n",
    "            _x = model(x)\n",
    "            \n",
    "            diff = cos(x, _x).cpu().tolist()\n",
    "            batch_pred = np.where(np.array(diff)<thr, 1,0).tolist()\n",
    "            pred += batch_pred\n",
    "    return pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d3a9562b4cd7c3ad0d08ad9b8620f31dce258d0003dd8d12b9da203ced86495d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
